version: 1
pruners:
    conv1_pruner:
        class: AutomatedGradualPruner
        initial_sparsity: 0.05
        final_sparsity: 0.8
        weights: [encoder.encoder.0.0.convs.0.0.float_weight,
                    encoder.encoder.1.0.skipconv.0.float_weight,
                    encoder.encoder.1.0.convs.0.0.float_weight,
                    encoder.encoder.1.0.convs.1.0.float_weight, 
                    encoder.encoder.2.0.skipconv.0.float_weight,
                    encoder.encoder.2.0.convs.0.0.float_weight, 
                    encoder.encoder.2.0.convs.1.0.float_weight, 
                    encoder.encoder.3.0.skipconv.0.float_weight,
                    encoder.encoder.3.0.convs.0.0.float_weight, 
                    encoder.encoder.3.0.convs.1.0.float_weight, 
                    encoder.encoder.4.0.skipconv.0.float_weight,
                    encoder.encoder.4.0.convs.0.0.float_weight, 
                    encoder.encoder.4.0.convs.1.0.float_weight,
                    encoder.encoder.5.0.convs.0.0.float_weight, 
                    encoder.encoder.5.0.convs.1.0.float_weight, 
                    prediction.head.0.0.float_weight,
                    prediction.head.2.0.float_weight,
                    prediction.head.4.0.float_weight]





#######################################################
quantizers:
    linear_quantizer:
        class: QuantAwareTrainRangeLinearQuantizer
        bits_activations: 8
        bits_weights: 8
        bits_bias: 8
        #bits_accum: 28
        #bits_sca
        mode: 'SYMMETRIC'
        ema_decay: 0.999
        per_channel_wts: True
        quantize_inputs: True
        num_bits_inputs: 8
        overrides:
            #conv1:
                #bits_weights: null
                #bits_activations: null
                #bits_bias: null
                #bits_accum: null
            #relu1:
                #bits_weights: null
                #bits_activations: null
                #bits_bias: null
                #bits_accum: null
                                             

policies:
    - pruner:
          instance_name: conv1_pruner
          args: 
            use_double_copies: [encoder.encoder.0.0.convs.0.0.float_weight,
                    encoder.encoder.1.0.skipconv.0.float_weight,
                    encoder.encoder.1.0.convs.0.0.float_weight,
                    encoder.encoder.1.0.convs.1.0.float_weight, 
                    encoder.encoder.2.0.skipconv.0.float_weight,
                    encoder.encoder.2.0.convs.0.0.float_weight, 
                    encoder.encoder.2.0.convs.1.0.float_weight, 
                    encoder.encoder.3.0.skipconv.0.float_weight,
                    encoder.encoder.3.0.convs.0.0.float_weight, 
                    encoder.encoder.3.0.convs.1.0.float_weight, 
                    encoder.encoder.4.0.skipconv.0.float_weight,
                    encoder.encoder.4.0.convs.0.0.float_weight, 
                    encoder.encoder.4.0.convs.1.0.float_weight,
                    encoder.encoder.5.0.convs.0.0.float_weight, 
                    encoder.encoder.5.0.convs.1.0.float_weight, 
                    prediction.head.0.0.float_weight,
                    prediction.head.2.0.float_weight,
                    prediction.head.4.0.float_weight]
      starting_epoch: 0
      ending_epoch: 330
      frequency: 3



###################################################
    - quantizer:
          instance_name: linear_quantizer
      starting_epoch: 0
      ending_epoch: 700
      frequency: 1





